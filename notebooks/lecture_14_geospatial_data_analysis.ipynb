{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b40ef5a4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# CYPLAN255\n",
    "### Urban Informatics and Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adc2c5d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "HIT RECORD and TRANSCRIBE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ceea7a8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 14 -- Geospatial Data Analysis\n",
    "******\n",
    "March 14, 2022\n",
    "\n",
    "<img src=\"https://s3-eu-west-1.amazonaws.com/ngs-mw-prod/9/97/11799/11799.jpg\" width=500 align='right' title='Alfred G Buckham, Aerial View of Edinburgh (c. 1920)'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f7d376",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Agenda\n",
    "1. Announcements\n",
    "2. Final projects\n",
    "3. An example\n",
    "4. More Geopandas\n",
    "5. Summary\n",
    "6. For next time\n",
    "7. Questions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd97b75a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1. Announcements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5d336f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Guest lecture on Wednesday (Irene!)\n",
    "- Still holding drop-in office hours today (remote)\n",
    "- Final project groups [sign-up sheet](https://docs.google.com/spreadsheets/d/1j9QWr_izCnWX48Q_AM0pzyap-sHCAuRMnJDqQNJJ9xs/edit?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c9fa3b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 2. Final Projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd2ca4c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame(\"../misc/final_project_description.pdf\", width=\"100%\", height=700)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f1753c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 3. An example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5360ebc7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Historical Redlining Is Associated with Present-Day Air Pollution Disparities in U.S. Cities**[<sup id=\"fn1-back\">1</sup>](#fn1)\n",
    "\n",
    "<img src=\"https://pubs.acs.org/na101/home/literatum/publisher/achs/journals/content/estlcu/0/estlcu.ahead-of-print/acs.estlett.1c01012/20220301/images/large/ez1c01012_0003.jpeg\" width=700>\n",
    "\n",
    "Two open data sets:\n",
    "  - Census Block-level air pollution data available from [CACES](https://www.caces.us/data)\n",
    "  - Redlining data available from https://dsl.richmond.edu/panorama/redlining/\n",
    "\n",
    "[<sup id=\"fn1\">1</sup>](#fn1-back) Lane, H. M., Morello-Frosch, R., Marshall, J. D., & Apte, J. S. (n.d.).  Environmental Science & Technology Letters, 0(0), null. https://doi.org/10.1021/acs.estlett.1c01012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bce4e48",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<iframe src=\"https://dsl.richmond.edu/panorama/redlining/\" width=100% height=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fe2528",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Question**\n",
    "\n",
    "- Correlation or causation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba52181e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- How might we inch our way towards causation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7f96f3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 4. More GeoPandas - Berkeley Edition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0691b780",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For this exercise we're going to download the Census Block geometries for the City of Berkeley:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4d32ad",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "blocks = gpd.read_file(\"https://data.cityofberkeley.info/api/geospatial/caxd-afre?method=export&format=Shapefile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b962b7e7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "blocks.plot(color='none', edgecolor='gray', linewidth=.2, figsize=(14,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65738a78",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4.1 Our First Geoprocessing Step: Dissolve\n",
    "\n",
    "Let's say we want to create a census tract dataset from the census block one, which we can see from inspecting the data above, contains a column called tractce10, for 2010 census tracts.  We can verify it by looking more closely at the geoid10 field, which appears to be a FIPS code.\n",
    "\n",
    "Geoprocessing steps like dissolve do geometric processing on the geometry column of a GeoSeries or GeoDataFrame (which contains a GeoSeries column)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd1afd5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "blocks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4887f59",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "tracts = blocks.dissolve(by='tractce10')\n",
    "tracts.plot(figsize=(10,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cebab16",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4.2 Setting the crs of GeoDataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e30034c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's see what the crs is for the newly created dataset, and what it's type is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfd5e77",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(tracts.crs)\n",
    "print(type(tracts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5675ac4c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "OK, so dissolve created a new GeoDataFrame, and assigned it the same crs as the original GeoDataFrame. But just to be sure you know how to do this in the event you load a file and the crs is undefined, we will set the crs in order to do other spatial operations on the tract dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1d2abf",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "tracts.crs = {'init' :'epsg:4326'}\n",
    "# or alternatively:\n",
    "tracts.crs = blocks.crs\n",
    "print(tracts.crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d95d680",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4.3. Loading 311 Cases for Grafitti and Vandalism as Point Data\n",
    "\n",
    "Let's load 311 data for graffiti and vandalism, dropping rows that have missing data (a good fraction of the data seem to be missing latitude and longitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60aeb703",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "vandalism = pd.read_json(\"https://data.cityofberkeley.info/resource/bscu-qpbu.json?request_category=Graffiti%20and%20Vandalism&$limit=20000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7259c6b1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "vandalism.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ca7a77",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We loaded a csv into a standard pandas DataFrame.  But it contains Latitude, Longitude columns, so with a couple of additional steps we can turn this into a GeoDataFrame, and set its crs.\n",
    "\n",
    "Study this example carefully. It is very common to encounter spatial data in a tabular form that you'll want to quickly turn into a geometry data types.  Note the use of the built-in `zip()` function within a list comprehension to pull the Latitude and Longitude columns together to define each Point geometry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beeb7ad9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from shapely.geometry import Point\n",
    "geometry = [Point(xy) for xy in zip(vandalism.longitude, vandalism.latitude)]\n",
    "geovandalism = gpd.GeoDataFrame(vandalism, crs='epsg:4326', geometry=geometry)\n",
    "geovandalism.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944f5ce2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4.4. Mapping With Layers\n",
    "\n",
    "Let's see what we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154cacfd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "geovandalism.plot(markersize=3);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ea50a8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A bunch of points mapped in isolation is not very informative, since it lacks context.  Let's add the points to the block base to add visual context. Note the use of Matplotlib syntax to set the parameters we want for the figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8452236",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(18,10))\n",
    "blocks.plot(color='white', edgecolor='black', linewidth=.1, ax=ax)\n",
    "geovandalism.plot(markersize=4, ax=ax)\n",
    "ax.set_title('Vandalism in Berkeley')\n",
    "ax.set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05d390c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4.5 Projecting to a Different crs\n",
    "\n",
    "So far our data has been in world coordinates (sometimes referred to as geographic cordinates). This works fine for some purposes like generating basic maps, other than the fact that the maps can appear distorted at different scales because we are squashing a spherical coordinate system onto a flat image. But there is a more fundamental issue: distances measured on a spherical coordinate system will be incorrect for point to point distance measurements we often want to do in urban data analysis.  Since we will be doing some spatial analysis on these data, we need to project them to a coordinate system that allows more meaningful measurements of distance. We'll use a standard Bay Area CRS for this purpose: EPSG:26910. You can find out more about this CRS [here](http://spatialreference.org/ref/sr-org/6787/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75644e07",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "geovandalism_proj = geovandalism.to_crs(\"epsg:26910\")\n",
    "blocks_proj = blocks.to_crs(\"epsg:26910\")\n",
    "tracts_proj = tracts.to_crs(\"epsg:26910\")\n",
    "tracts_proj.crs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa64c807",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's check the bounds for the whole dataset to see what kind of coordinates we're working with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffc6e15",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "bounds = tracts_proj.total_bounds\n",
    "print(bounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f01a5e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4.6 Spatial Join: Intersect\n",
    "\n",
    "A very common geoprocessing operation is to do a **point-in-polygon** assignment using an intersection of the geometries of points and polygons.  It is like a normal merge, where the two datasets do not have a common key to merge on, but instead these tables have coordinates that enable geometric processing to find matching rows.\n",
    "\n",
    "GeoPandas makes this pretty easy using a spatial join, or `gpd.sjoin()` function with the `intersects` operation argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91977c05",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "geovandalism_proj_blocks = gpd.sjoin(geovandalism_proj, blocks_proj, how=\"inner\", op='intersects')\n",
    "geovandalism_proj_blocks.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcef8e45",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now we know the Census Block ID of each record in the vandalism data!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429e146e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4.7 Aggregating data using Groupby\n",
    "\n",
    "Let's say we want to look at the incidence of vandalism events at some level of geography like neighborhood or census tract rather than just looking at the raw data.  We can use groupby operations to get the counts of events within each geographic area, then merge it on to a Geodataframe to visualize it.\n",
    "\n",
    "We use a reset_index to create a unique index and make the groupby column become a column in the resulting dataframe instead of the index. That makes the merge a bit clearer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673505f6",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "tract_v = geovandalism_proj_blocks.groupby('tractce10')['case_id'].count().to_frame(name='total_vandalism').reset_index()\n",
    "tract_v.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a64668",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "tracts2 = pd.merge(tracts_proj,tract_v, left_index=True, right_on='tractce10')\n",
    "tracts2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b67f40",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4.8. Choropleth Maps\n",
    "\n",
    "One of the more obvious things you might want to do with spatial data is visualize the variations in it spatially.  Besides plotting the individual points to see their pattern, we can generate a choropleth, or thematic, map by census tract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486914f2",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "tracts2.plot(column='total_vandalism', figsize=(14,10), legend=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f45aa6d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The default colormap is not very useful in this case. We can select a better colormap with the `cmap` param to  make our map more easily interpretable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee658c51",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "tracts2.plot(column='total_vandalism', cmap='YlGn', figsize=(14,10), legend=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07dfaf37",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now might be a good opportunity to spend a few minutes experimenting with different colormaps. A good place to start would be here: http://matplotlib.org/users/colormaps.html\n",
    "\n",
    "Which kinds of colormaps seem to work best for this data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1247da",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4.9. The Modifiable Areal Unit Problem (MAUP)\n",
    "\n",
    "One common issue when working with geospatial data is known as the Modifiable Areal Unit Problem (MAUP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c4440d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"https://mikejohnson51.github.io/spds/lec-img/15-maup.png\" width=600></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ee644f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This comes up quite frequently when working with choropleth maps, since polygons with larger areas tend to show up more prominently than might be appropriate. This happens for two reasons:\n",
    "  1. Large areas occupy more visual space on the map\n",
    "  2. Larger area == more observations \n",
    "\n",
    "We'll now investigate a few ways for dealing with these issues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e78724",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 4.9.1. Use Equal-Area Bins\n",
    "Perhaps the simplest approach would have been to just leave the Census Tracts out of it for the purpose of data viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d428b83b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# code won't work if we have any empty geometries\n",
    "geovandalism_proj_blocks = geovandalism_proj_blocks[~geovandalism_proj_blocks.is_empty]\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots(figsize=(15,15))\n",
    "g = blocks_proj.plot(color='none', edgecolor='k', linewidth=.25, ax=ax)\n",
    "hb = ax.hexbin(geovandalism_proj_blocks['geometry'].x, geovandalism_proj_blocks['geometry'].y, gridsize=20, mincnt=1)\n",
    "fig.colorbar(hb, shrink=.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e357ba",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 4.9.2. Normalize by Area\n",
    "\n",
    "We often want to normalize our data by area to compute a density to offset this. Geopandas gives us access to an attribute of the geometry of polygons to get the areas, so this is fairly straightforward to do.\n",
    "\n",
    "But first, check to see what units our projection is in so we can convert to a known area size, like square miles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3731f27",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "tracts2.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a123f2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "tracts2['area_sqmi'] = tracts2.area / 3.861e-7\n",
    "tracts2['van_sqmi'] = tracts2['total_vandalism'] / tracts2['area_sqmi']\n",
    "tracts2.plot(column='van_sqmi', cmap='OrRd', figsize=(10,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b8f2d5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Ah, that looks much better!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2d4416",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 4.9.3. Interpolate!\n",
    "\n",
    "If we want to avoid aggregating the data into somewhat arbitrary geographies like census tracts, we can use 2D kernel density estimation to convert our discrete data points into a continuous surface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ed85d5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,15))\n",
    "g = blocks_proj.plot(color='none', edgecolor='k', linewidth=.25, ax=ax)\n",
    "hb = sns.kdeplot(\n",
    "    x=geovandalism_proj_blocks['geometry'].x, y=geovandalism_proj_blocks['geometry'].y,\n",
    "    cmap='turbo', fill=True, alpha=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a452249",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The code in the next cell does essentially the same thing as the 2D KDE, but instead of estimating a continuous function it estimates a discrete 2D _histogram_ and applies a smoothing function. Code is adapted from [here](http://nbviewer.jupyter.org/gist/perrygeo/c426355e40037c452434)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4d6f3e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from scipy import ndimage\n",
    "import numpy as np\n",
    "\n",
    "def heatmap(d, bins=(100,100), smoothing=1.3, cmap='turbo'):\n",
    "    def getx(pt):\n",
    "        return pt.coords[0][0]\n",
    "\n",
    "    def gety(pt):\n",
    "        return pt.coords[0][1]\n",
    "\n",
    "    x = list(d.geometry.apply(getx))\n",
    "    y = list(d.geometry.apply(gety))\n",
    "    heatmap, xedges, yedges = np.histogram2d(y, x, bins=bins)\n",
    "    extent = [yedges[0], yedges[-1], xedges[-1], xedges[0]]\n",
    "\n",
    "    logheatmap = np.log(heatmap)\n",
    "    logheatmap[np.isneginf(logheatmap)] = 0\n",
    "    logheatmap = ndimage.gaussian_filter(logheatmap, smoothing, mode='nearest')\n",
    "    blocks_proj.plot(color='none', edgecolor='white', linewidth=.5, alpha=.5, figsize=(15,10))\n",
    "\n",
    "    plt.imshow(logheatmap, cmap=cmap, extent=extent)\n",
    "    plt.colorbar(shrink=.75)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.show()\n",
    "\n",
    "heatmap(geovandalism_proj_blocks, bins=190, smoothing=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9067880d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Your Turn\n",
    "\n",
    "To practice with Geopandas, experiment with the methods covered so far with data you are interested in working with on your project, or any other data you can find readily from an Open Data Portal like Berkeley, San Francisco, Oakland, New York, or others.\n",
    "\n",
    "* Download a shapefile containing point data and attributes\n",
    "* Create a GeoDataFrame\n",
    "* Set its CRS\n",
    "* Plot it with color coding of the points based on the values of an attribute\n",
    "* Download a shapefile containing polygons and attributes\n",
    "* Create a GeoDataFrame\n",
    "* Plot a Choroplethic Map\n",
    "* Change the coordinate procjetion on these from spherical to a projected coordinate system\n",
    "* Do a spatial join of the point and polygon data\n",
    "* Aggregate the joined data to summarize it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffbf6f6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 5. For next time\n",
    "- Python installs: PySAL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea0b2f7",
   "metadata": {
    "hide_input": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 6. Questions?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cd044d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Sources\n",
    "\n",
    "- This notebook was heavily adapted from previous course material by [Prof. Paul Waddell](https://urbansim.com/people)."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:cp255] *",
   "language": "python",
   "name": "conda-env-cp255-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
